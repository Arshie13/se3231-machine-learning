{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.linear_model import LinearRegression  \n",
    "from sklearn.metrics import mean_squared_error, r2_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/prelim_datasets/gameandgrade.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the double dots from a certain entry from the Grade column (very sneaky)\n",
    "\n",
    "df[\"Grade\"] = df[\"Grade\"].str.replace(r\"\\.\\.\", \".\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[[\"Playing Hours\"]]\n",
    "y = df[\"Grade\"]\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Intercept (b): {model.intercept_:.2f}\")  # The base score when all features are 0\n",
    "coefficients = pd.DataFrame(model.coef_, x.columns, columns=['Coefficient'])\n",
    "print(coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"RÂ² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot\n",
    "plt.figure(figsize=(40, 20))\n",
    "sns.scatterplot(x=y_test.values.flatten(), y=y_test.values.flatten(), color=\"blue\", label=\"Actual\", alpha=0.6)  # Actual values\n",
    "sns.scatterplot(x=y_test.values.flatten(), y=y_pred.flatten(), color=\"red\", label=\"Predicted\", alpha=0.6)  # Predicted values\n",
    "\n",
    "# Plot a reference diagonal line (perfect predictions)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color=\"black\", linestyle=\"--\", label=\"Perfect Fit\")\n",
    "\n",
    "# Labels, title, and legend\n",
    "plt.xlabel(\"Actual Assessment of Grade\")\n",
    "plt.ylabel(\"Predicted Assessment of Grade\")\n",
    "plt.title(\"Actual vs. Predicted Assessment of Grade\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
